{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdamJuma133/C-P-Disease-Identifier-Project/blob/main/Copy_of_Getting_Started_with_Deep_Lake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKU8kmSs65xv"
      },
      "source": [
        "# **Step 1**: _Hello World_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrjGQON37lk2"
      },
      "source": [
        "## Installing Deep Lake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pcfYcPu7KxY"
      },
      "source": [
        "Deep Lake can be installed via `pip`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oC_N5qOx6o0d"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!pip3 install deeplake\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart the kernel if Colab is on an old version of pillow, so that the new version that was installed with Deep Lake is used\n",
        "from packaging.version import Version\n",
        "import PIL\n",
        "\n",
        "if Version(PIL.__version__) < Version(\"10.2.0\"):\n",
        "  raise Exception('Please restart your kernel and proceed with the cell below, to ensure that the version of Pillow corresponds to the version installed with Deep Lake')"
      ],
      "metadata": {
        "id": "xPrOSbzTrFzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfU05eotvmby"
      },
      "outputs": [],
      "source": [
        "# Temporary fix for using Deep Lake Storage in Colab\n",
        "with open('/etc/resolv.conf', 'w') as file:\n",
        "  file.write(\"nameserver 8.8.8.8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4_rfJ_GVxLz"
      },
      "source": [
        "**By default, Deep Lake does not install dependencies for audio, video, google-cloud, and other features. Details on installation options are available [here](https://docs.deeplake.ai/en/latest/Installation.html).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N-f2SYU7OjQ"
      },
      "source": [
        "## Fetching your first Deep Lake dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aNFn7rZ7qxP"
      },
      "source": [
        "Begin by loading in [MNIST](https://en.wikipedia.org/wiki/MNIST_database), the hello world dataset of machine learning.\n",
        "\n",
        "First, load the `Dataset` by pointing to its storage location. Datasets hosted on Activeloop are typically identified by the namespace of the organization followed by the dataset name: `activeloop/mnist-train`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izccjS4k7NvX",
        "outputId": "fa4dced7-8a1a-44cc-dcea-7d0309485058",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "The API for Deep Lake 4.0 has changed significantly, including the `load` method being replaced by `open`. To continue using Deep Lake 3.x, use `pip install \"deeplake<4\"`. For information on migrating your code, see https://docs.deeplake.ai/latest/details/v3_conversion/",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-1f5ad48b1222>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'hub://activeloop/mnist-train'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeeplake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Returns a Deep Lake Dataset but does not download data locally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deeplake/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;36m.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m     raise Exception(\n\u001b[0m\u001b[1;32m    168\u001b[0m         \"\"\"\n\u001b[1;32m    169\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mAPI\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mDeep\u001b[0m \u001b[0mLake\u001b[0m \u001b[0;36m4.0\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mchanged\u001b[0m \u001b[0msignificantly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincluding\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mload\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0mbeing\u001b[0m \u001b[0mreplaced\u001b[0m \u001b[0mby\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: The API for Deep Lake 4.0 has changed significantly, including the `load` method being replaced by `open`. To continue using Deep Lake 3.x, use `pip install \"deeplake<4\"`. For information on migrating your code, see https://docs.deeplake.ai/latest/details/v3_conversion/"
          ]
        }
      ],
      "source": [
        "import deeplake\n",
        "\n",
        "dataset_path = 'hub://activeloop/mnist-train'\n",
        "ds = deeplake.load(dataset_path) # Returns a Deep Lake Dataset but does not download data locally"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR5n8yYg-0Wu"
      },
      "source": [
        "## Reading Samples From a Deep Lake Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XdaAKaS-3NO"
      },
      "source": [
        "Data is not immediately read into memory because Deep Lake operates [lazily](https://en.wikipedia.org/wiki/Lazy_evaluation). You can fetch data by calling the `.numpy()` method, which reads data into a NumPy array.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qpQeNoq-xfo"
      },
      "outputs": [],
      "source": [
        "# Indexing\n",
        "img = ds.images[0].numpy()              # Fetch the 1st image and return a NumPy array\n",
        "label = ds.labels[0].numpy(aslist=True) # Fetch the 1st label and store it as a\n",
        "                                        # as a list\n",
        "\n",
        "text_labels = ds.labels[0].data()['text'] # Fetch the first labels and return them as text\n",
        "\n",
        "# Slicing\n",
        "imgs = ds.images[0:100].numpy() # Fetch 100 images and return a NumPy array\n",
        "                                # The method above produces an exception if\n",
        "                                # the images are not all the same size\n",
        "\n",
        "labels = ds.labels[0:100].numpy(aslist=True) # Fetch 100 labels and store\n",
        "                                             # them as a list of NumPy arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNGHXfdKwJ7W"
      },
      "outputs": [],
      "source": [
        "print('label is {}'.format(text_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmi2w0_e_LtH"
      },
      "source": [
        "Congratulations, you've got Deep Lake working on your local machine! ðŸ¤“"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X8u83XqLJfLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T2uSWYAgJgLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-DM6PKq_di2"
      },
      "source": [
        "# **Step 2**: _Creating Deep Lake Datasets_\n",
        "*Creating and storing Deep Lake Datasets.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEzK8LTe_gJW"
      },
      "source": [
        "This guide creates Deep Lake datasets locally. You may create datasets in the Activeloop cloud by [registering](https://app.activeloop.ai/register), creating an API token, and replacing the local paths below with the path to a dataset in your Deep Lake organization `hub://organization_name/dataset_name`\n",
        "\n",
        "You don't have to worry about uploading datasets after you've created them. They are automatically synchronized with [wherever they are being stored](https://docs.activeloop.ai/authentication-overview)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGXGvKU1qsp1"
      },
      "source": [
        "## Manual Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQk29Mnhqn1V"
      },
      "source": [
        "Let's follow along with the example below to create our first dataset. First, download and unzip the small classification dataset below called the *animals dataset*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDJRrlDP_DsW"
      },
      "outputs": [],
      "source": [
        "# Download dataset\n",
        "from IPython.display import clear_output\n",
        "!wget https://github.com/activeloopai/examples/raw/main/colabs/starting_data/animals.zip\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIQf9cY6_vyn"
      },
      "outputs": [],
      "source": [
        "# Unzip to './animals' folder\n",
        "!unzip -qq /content/animals.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIz-MYImAfCg"
      },
      "source": [
        "The dataset has the following folder structure:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuhZZqVIAqj_"
      },
      "source": [
        "animals\n",
        "- cats\n",
        "  - image_1.jpg\n",
        "  - image_2.jpg\n",
        "- dogs\n",
        "  - image_3.jpg\n",
        "  - image_4.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Lez5uCJAto4"
      },
      "source": [
        "Now that you have the data, you can **create a Deep Lake `Dataset`** and initialize its tensors. Running the following code will create a Deep Lake dataset inside of the `./animals_dl` folder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtzmT0iBNV23"
      },
      "outputs": [],
      "source": [
        "import deeplake\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "ds = deeplake.empty('./animals_dl') # Creates the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ5yt0aaNeP5"
      },
      "source": [
        "Next, let's inspect the folder structure for the source dataset `'./animals'` to find the class names and the files that need to be uploaded to the Deep Lake dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubGLkgG8Njbb"
      },
      "outputs": [],
      "source": [
        "# Find the class_names and list of files that need to be uploaded\n",
        "dataset_folder = './animals'\n",
        "\n",
        "# Find the subfolders, but filter additional files like DS_Store that are added on Mac machines.\n",
        "class_names = [item for item in os.listdir(dataset_folder) if os.path.isdir(os.path.join(dataset_folder, item))]\n",
        "\n",
        "files_list = []\n",
        "for dirpath, dirnames, filenames in os.walk(dataset_folder):\n",
        "    for filename in filenames:\n",
        "        files_list.append(os.path.join(dirpath, filename))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtVSh0FnNmyI"
      },
      "source": [
        "Next, let's **create the dataset tensors and upload metadata**. Check out our page on [Storage Synchronization](https://docs.activeloop.ai/how-it-works/storage-synchronization) for details about the `with` syntax below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6QDC6caNpiH"
      },
      "outputs": [],
      "source": [
        "with ds:\n",
        "  # Create the tensors with names of your choice.\n",
        "  ds.create_tensor('images', htype = 'image', sample_compression = 'jpeg')\n",
        "  ds.create_tensor('labels', htype = 'class_label', class_names = class_names)\n",
        "\n",
        "  # Add arbitrary metadata - Optional\n",
        "  ds.info.update(description = 'My first Deep Lake dataset')\n",
        "  ds.images.info.update(camera_type = 'SLR')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TD-hCSBKBA_m"
      },
      "source": [
        "**Note:** Specifying [htype](https://docs.activeloop.ai/dataset-visualization/data-type-htype) and `dtype` is not required, but it is highly recommended in order to optimize performance, especially for large datasets. Use `dtype` to specify the numeric type of tensor data, and use `htype` to specify the underlying data structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR4kLo6YBOhO"
      },
      "source": [
        "Finally, let's **populate the data** in the tensors.         "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QRAyS-HA-Fp"
      },
      "outputs": [],
      "source": [
        "with ds:\n",
        "    # Iterate through the files and append to Deep Lake dataset\n",
        "    for file in files_list:\n",
        "        label_text = os.path.basename(os.path.dirname(file))\n",
        "        label_num = class_names.index(label_text)\n",
        "\n",
        "        #Append data to the tensors\n",
        "        ds.append({'images': deeplake.read(file), 'labels': np.uint32(label_num)})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWqYzfI1DCPG"
      },
      "source": [
        "**Note:** `ds.append({'images': deeplake.read(path)})` is functionally equivalent to `ds.append({'images': PIL.Image.fromarray(path)})`. However, the `deeplake.read()` method is significantly faster because it does not decompress and recompress the image if the compression matches the `sample_compression` for that tensor. Further details are available in the next section.\n",
        "\n",
        "**Note:** In order to maintain proper indexing across tensors, `ds.append({...})` requires that you to append to all tensors in the dataset. If you wish to skip tensors during appending, please use `ds.append({...}, skip_ok = True)` or append to a single tensor using `ds.tensor_name.append(...)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzHVb521XSud"
      },
      "source": [
        "Check out the first label and image from this dataset. More details about Accessing Data are available in **Step 4**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCWJ1RqmSTQQ"
      },
      "outputs": [],
      "source": [
        "ds.labels.info.class_names[ds.labels[0].numpy()[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMG2oif0XSDZ"
      },
      "outputs": [],
      "source": [
        "Image.fromarray(ds.images[0].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPZReEhwu3Hx"
      },
      "source": [
        "You can print a summary of the dataset structure using:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWbOhUV_u9Wo"
      },
      "outputs": [],
      "source": [
        "ds.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8E_f-eXqy1c"
      },
      "source": [
        "## Automatic Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCjy5dH9q3Gi"
      },
      "source": [
        "If your source data conforms to one of the formats below, you can ingest them directly with 1 line of code.\n",
        "* YOLO\n",
        "* [COCO](https://docs.deeplake.ai/en/latest/deeplake.html#deeplake.ingest_coco)\n",
        "*Classifications\n",
        "\n",
        "For example, the above animals dataset can be converted to Deep Lake format using:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUtOL7F8q1xB"
      },
      "outputs": [],
      "source": [
        "src = \"./animals\"\n",
        "dest = './animals_dl_auto'\n",
        "\n",
        "ds = deeplake.ingest_classification(src, dest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6xboPUKrs1l"
      },
      "outputs": [],
      "source": [
        "Image.fromarray(ds.images[0].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03b3r7owq7o8"
      },
      "source": [
        "**Note**: Automatic creation currently supports image classification datasets where classes are separated by folder, though support for other dataset types is continually being added."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK_wpkYsDdH2"
      },
      "source": [
        "## Creating Tensor Hierarchies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1btlOtBDDe4G"
      },
      "source": [
        "Often it's important to create tensors hierarchically, because information between tensors may be inherently coupledâ€”such as bounding boxes and their corresponding labels. Hierarchy can be created using tensor `groups`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICg3Z1z8CRGN"
      },
      "outputs": [],
      "source": [
        "ds = deeplake.empty('./groups_test') # Creates the dataset\n",
        "\n",
        "# Create tensor hierarchies\n",
        "ds.create_group('my_group')\n",
        "ds.my_group.create_tensor('my_tensor')\n",
        "\n",
        "# Alternatively, a group can us created using create_tensor with '/'\n",
        "ds.create_tensor('my_group_2/my_tensor') # Automatically creates the group 'my_group_2'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE-rWBCkpI9T"
      },
      "source": [
        "Tensors in groups are accessed via:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78s3Oa_jpKXV"
      },
      "outputs": [],
      "source": [
        "ds.my_group.my_tensor\n",
        "\n",
        "#OR\n",
        "\n",
        "ds['my_group/my_tensor']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fhjWZ9hDvKe"
      },
      "source": [
        "For more detailed information regarding accessing datasets and their tensors, check out **Step 4**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46H4nEnZDv5m"
      },
      "source": [
        "# **Step 3**: _Understanding Compression_\n",
        "\n",
        "*Using compression to achieve optimal performance.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ajldDggEp8O"
      },
      "source": [
        "**Data in Deep Lake can be stored in raw uncompressed format. However, compression is highly recommended for achieving optimal performance in terms of speed and storage.**\n",
        "\n",
        "\n",
        "Compression is specified separately for each tensor, and it can occur at the `sample` or `chunk` level. For example, when creating a tensor for storing images, you can choose the compression technique for the image samples using the `sample_compression` input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOw9hc0jDpQY",
        "outputId": "4bf8f49e-d977-4520-bac8-a4e548787364",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "The API for Deep Lake 4.0 has changed significantly, including the `empty` method being replaced by `create`. To continue using Deep Lake 3.x, use `pip install \"deeplake<4\"`. For information on migrating your code, see https://docs.deeplake.ai/latest/details/v3_conversion/",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-909cc2d6843a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Set overwrite = True for re-runability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeeplake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./compression_test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'image'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_compression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'jpeg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deeplake/__init__.py\u001b[0m in \u001b[0;36mempty\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;36m.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \"\"\"\n\u001b[0;32m--> 182\u001b[0;31m     raise Exception(\n\u001b[0m\u001b[1;32m    183\u001b[0m         \"\"\"\n\u001b[1;32m    184\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mAPI\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mDeep\u001b[0m \u001b[0mLake\u001b[0m \u001b[0;36m4.0\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mchanged\u001b[0m \u001b[0msignificantly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincluding\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0mbeing\u001b[0m \u001b[0mreplaced\u001b[0m \u001b[0mby\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: The API for Deep Lake 4.0 has changed significantly, including the `empty` method being replaced by `create`. To continue using Deep Lake 3.x, use `pip install \"deeplake<4\"`. For information on migrating your code, see https://docs.deeplake.ai/latest/details/v3_conversion/"
          ]
        }
      ],
      "source": [
        "import deeplake\n",
        "\n",
        "# Set overwrite = True for re-runability\n",
        "ds = deeplake.empty('./compression_test', overwrite = True)\n",
        "\n",
        "ds.create_tensor('images', htype = 'image', sample_compression = 'jpeg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv4ktXoCE2K2"
      },
      "source": [
        "In this example, every image added in subsequent `.append(...)` calls is compressed using the specified `sample_compression` method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WaFBxrEE9GI"
      },
      "source": [
        "### **Choosing the Right Compression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM8VtZ98FCUu"
      },
      "source": [
        "There is no single answer for choosing the right compression, and the tradeoffs are described in detail in the next section. However, good rules of thumb are:\n",
        "\n",
        "\n",
        "\n",
        "1.   For data that has application-specific compressors (`image`, `audio`, `video`,...), choose the sample_compression technique that is native to the application such as `jpg`, `mp3`, `mp4`,...\n",
        "2.   For other data containing large samples (i.e. large arrays with >100 values), `lz4` is a generic compressor that works well in most applications. `lz4` can be used as a `sample_compression` or `chunk_compression`. In most cases, `sample_compression` is sufficient, but in theory, `chunk_compression` produces slightly smaller data.\n",
        "3.   For other data containing small samples (i.e. labels with <100 values), it is not necessary to use compression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hotuAwslFbAu"
      },
      "source": [
        "### **Compression Tradeoffs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrWvN558v4xn"
      },
      "source": [
        "**Lossiness -** Certain compression techniques are lossy, meaning that there is irreversible information loss when compressing the data. Lossless compression is less important for data such as images and videos, but it is critical for label data such as numerical labels, binary masks, and segmentation data.\n",
        "\n",
        "\n",
        "**Memory -** Different compression techniques have substantially different memory footprints. For instance, png vs jpeg compression may result in a 10X difference in the size of a Deep Lake dataset.\n",
        "\n",
        "\n",
        "**Runtime -** The primary variables affecting download and upload speeds for generating usable data are the network speed and available compute power for processing the data . In most cases, the network speed is the limiting factor. Therefore, the highest end-to-end throughput for non-local applications is achieved by maximizing compression and utilizing compute power to decompress/convert the data to formats that are consumed by deep learning models (i.e. arrays).\n",
        "\n",
        "\n",
        "**Upload Considerations -** When applicable, the highest uploads speeds can be achieved when the  `sample_compression` input matches the compression of the source data, such as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkJKv00UFexo"
      },
      "outputs": [],
      "source": [
        "# sample_compression is \"jpg\" and appended image is \"jpeg\"\n",
        "ds.create_tensor('images_jpg', htype = 'image', sample_compression = 'jpg')\n",
        "ds.images_jpg.append(deeplake.read('./animals/dogs/image_3.jpg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LMsd3K9GJJ9"
      },
      "source": [
        "In this case, the input data is a `.jpg`, and the Deep Lake `sample_compression` is `jpg`.\n",
        "\n",
        "However, a mismatch between compression of the source data and sample_compression in Deep Lake results in significantly slower upload speeds, because Deep Lake must decompress the source data and recompress it using the specified `sample_compression` before saving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5MaC1lBwa3a"
      },
      "outputs": [],
      "source": [
        "# sample_compression is \"jpg\" and appended image is \"jpeg\"\n",
        "ds.create_tensor('images_png', htype = 'image', sample_compression = 'png')\n",
        "ds.images_png.append(deeplake.read('./animals/dogs/image_3.jpg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXGCWxu7wjHX"
      },
      "source": [
        "**NOTE:** Due to the computational costs associated with decompressing and recompressing data, it is important that you consider the runtime implications of uploading source data that is compressed differently than the specified sample_compression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGo-E8Z8Ho6F"
      },
      "source": [
        "# **Step 4**: _Accessing Data_\n",
        "_Accessing and loading Deep Lake Datasets._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8Mye_Z5Htut"
      },
      "source": [
        "## Loading Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DI_D7flHvEN"
      },
      "source": [
        "Deep Lake Datasets can be loaded and created in a variety of storage locations with minimal configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9dl3mfENulO"
      },
      "outputs": [],
      "source": [
        "import deeplake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sltdan65HmRN"
      },
      "outputs": [],
      "source": [
        "# Local Filepath\n",
        "ds = deeplake.load('./animals_dl') # Dataset created in Step 2 in this Colab Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41FBvx25NWMN"
      },
      "outputs": [],
      "source": [
        "# S3\n",
        "# ds = deeplake.load('s3://my_dataset_bucket', creds={...})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuacdMOgNNmT"
      },
      "outputs": [],
      "source": [
        "# Public Dataset hosted by Activeloop\n",
        "## Activeloop Storage - See Step 6\n",
        "ds = deeplake.load('hub://activeloop/k49-train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocs18sNqNQfG"
      },
      "outputs": [],
      "source": [
        "# Dataset in another organization on Activeloop\n",
        "# ds = deeplake.load('hub://organization_name/dataset_name')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD60qFaAH2qg"
      },
      "source": [
        "**Note:** Since `ds = deeplake.dataset(path)` can be used to both create and load datasets, you may accidentally create a new dataset if there is a typo in the path you provided while intending to load a dataset. If that occurs, simply use `ds.delete()` to remove the unintended dataset permanently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Kb9q_ZqIARN"
      },
      "source": [
        "## Referencing Tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq5WSI5LIClV"
      },
      "source": [
        "Deep Lake allows you to reference specific tensors using keys or via the `.` notation outlined below.\n",
        "\n",
        "\n",
        "**Note:** data is still not loaded by these commands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jr_ZEtBnN1Wp"
      },
      "outputs": [],
      "source": [
        "ds = deeplake.dataset('hub://activeloop/k49-train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24trRqlLH0Tl"
      },
      "outputs": [],
      "source": [
        "### NO HIERARCHY ###\n",
        "ds.images # is equivalent to\n",
        "ds['images']\n",
        "\n",
        "ds.labels # is equivalent to\n",
        "ds['labels']\n",
        "\n",
        "### WITH HIERARCHY ###\n",
        "# ds.localization.boxes # is equivalent to\n",
        "# ds['localization/boxes']\n",
        "\n",
        "# ds.localization.labels # is equivalent to\n",
        "# ds['localization/labels']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjmnRLWHINXG"
      },
      "source": [
        "## Accessing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js3jsmBHIPqu"
      },
      "source": [
        "Data within the tensors is loaded and accessed using the `.numpy()` command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QUWjQNGILWQ"
      },
      "outputs": [],
      "source": [
        "# Indexing\n",
        "img = ds.images[0].numpy()              # Fetch the 1st image and return a NumPy array\n",
        "label = ds.labels[0].numpy(aslist=True) # Fetch the 1st label and store it as a\n",
        "                                        # as a list\n",
        "\n",
        "# frame = ds.videos[0][4].numpy()   # Fetch the 5th frame in the 1st video\n",
        "                                    # and return a NumPy array\n",
        "\n",
        "text_labels = ds.labels[0].data()['value'] # Fetch the first labels and return them as text\n",
        "\n",
        "# Slicing\n",
        "imgs = ds.images[0:100].numpy() # Fetch 100 images and return a NumPy array\n",
        "                                # The method above produces an exception if\n",
        "                                # the images are not all the same size\n",
        "\n",
        "labels = ds.labels[0:100].numpy(aslist=True) # Fetch 100 labels and store\n",
        "                                             # them as a list of NumPy arrays"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DykgrsBEIfk1"
      },
      "source": [
        "**Note:** The `.numpy()` method will produce an exception if all samples in the requested tensor do not have a uniform shape. If that's the case, running `.numpy(aslist=True)` solves the problem by returning a list of NumPy arrays, where the indices of the list correspond to different samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDlGbLdPPW30"
      },
      "source": [
        "## Updating Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klccwzX3PdGK"
      },
      "outputs": [],
      "source": [
        "ds = deeplake.load('./animals_dl') # Dataset created in Step 2 in this Colab Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVtQKt7ZPgH4"
      },
      "source": [
        "Existing data in a Deep Lake dataset can be updated using:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82O-4wnBPhAL"
      },
      "outputs": [],
      "source": [
        "ds.images[1] = deeplake.read('https://i.postimg.cc/Yq2SNz9J/photo-1534567110243-8875d64ca8ff.jpg') # If the URI is not public, credentials should be specified using deeplake.read(URI, creds = {...})\n",
        "ds.labels[1] = 'giraffe' # Tensors of htype = 'class_label' can be updated with either numeric values or text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pOmxXkiPk4V"
      },
      "outputs": [],
      "source": [
        "Image.fromarray(ds.images[1].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K385Fpvmqc0l"
      },
      "source": [
        "#**Step 5**: *Visualizing Datasets*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSIK-TCAqqQF"
      },
      "source": [
        "One of Deep Lake's core features is to enable users to visualize and interpret large amounts of data. Let's load the COCO dataset, which is one of the most popular datasets in computer vision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YRBC6ehqpgz"
      },
      "outputs": [],
      "source": [
        "import deeplake\n",
        "\n",
        "ds = deeplake.load('hub://activeloop/coco-train')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5TW4f4Zqzlw"
      },
      "source": [
        "The tensor layout for this dataset can be inspected using:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YU10NNvNqz54"
      },
      "outputs": [],
      "source": [
        "ds.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StDTRjIJq3qI"
      },
      "source": [
        "The dataset can be [visualized in the UI](https://app.activeloop.ai/activeloop/coco-train), or using an iframe in a jupyter notebook. If you don't already have flask and ipython installed, make sure to install Deep Lake using `pip install deeplake[visualizer]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7G3X22Tdq5tn"
      },
      "outputs": [],
      "source": [
        "ds.visualize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L713rdfJtVKS"
      },
      "source": [
        "**Note:** Visualizing datasets in [Activeloop UI](https://app.activeloop.ai/) will unlock more features and faster performance compared to visualization in Jupyter notebooks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul8Q0CK6rH50"
      },
      "source": [
        "##Visualizing your own datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQzmJazJrOaF"
      },
      "source": [
        "Any Deep Lake dataset can be visualized using the methods above as long as it follows the conventions necessary for the visualization engine to interpret and parse the data. These conventions [are explained here](https://docs.activeloop.ai/dataset-visualization)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQipSo2OF_lB"
      },
      "source": [
        "# **Step 6**: _Using Activeloop Storage_\n",
        "\n",
        "_Storing and loading datasets from Activeloop Storage._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TJfXx2pgG7P"
      },
      "source": [
        "## Register and Authenticate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA39G647GHX4"
      },
      "source": [
        "You can store your Deep Lake Datasets with Activeloop by first creating an account in the [Deep Lake App](https://app.activeloop.ai/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EagHjbCswdcj"
      },
      "source": [
        "Then should set the environmental variable `ACTIVELOOP_TOKEN` in your environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_vuqcRZwjHl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ['ACTIVELOOP_TOKEN'] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvBxhaAYGNOi"
      },
      "source": [
        "You can then access or create Deep Lake Datasets by passing the Activeloop path to methods such as `deeplake.load()`, `deeplake.empty()`, and others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeL0a2zwGXeU"
      },
      "outputs": [],
      "source": [
        "import deeplake\n",
        "\n",
        "org_id = <your_org> # You already have an org that shares your username\n",
        "dataset_name = <dataset_name>\n",
        "\n",
        "ds = deeplake.dataset(f\"hub://{org_id}/{dataset_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huQQ1M8kGcyL"
      },
      "source": [
        "**Note**: When you create an account in Activeloop, a default organization is created that has the same name as your username. You can also create other organization that represent companies, teams, or other collections of multiple users."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUdVLQUGGnsA"
      },
      "source": [
        "Public datasets such as `hub://activeloop/mnist-train` can be accessed without logging in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVma__gxGq97"
      },
      "source": [
        "# **Step 7**: _Connecting Deep Lake Datasets to ML Frameworks_\n",
        "\n",
        "_Connecting Deep Lake Datasets to machine learning frameworks such as PyTorch and TensorFlow._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8r-AkeJMGwxB"
      },
      "source": [
        "Deep Lake Datasets can be connected to popular ML frameworks such as PyTorch and TensorFlow using minimal boilerplate code. Our methods enable you to train models while streaming data from the cloud without bottlenecking the training process!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bnr9ItdkGzDk"
      },
      "source": [
        "## PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IEBtwtU5wdq"
      },
      "source": [
        "There are two syntaxes that can be used to train models in Pytorch using Deep Lake datasets:\n",
        "\n",
        "\n",
        "1.   **Deep Lake Data Loaders** are highly-optimized and unlock the fastest streaming and shuffling using Deep Lake's internal shuffling method. However, they do not support custom sampling or fully-random shuffling that is possible using PyTorch datasets + data loaders.\n",
        "2.   **Pytorch Datasets + Data Loaders** enable all the customizability supported by PyTorch. However, they have highly sub-optimal streaming using Deep Lake datasets and may result in 5X+ slower performance compared to using Deep Lake data loaders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNPlnk7V58qE"
      },
      "source": [
        "### **Using Deep Lake Data Loaders**\n",
        "\n",
        "**Best option for fast streaming!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKkrCv2NG1GG"
      },
      "source": [
        "The fastest streaming of data to GPUs using PyTorch is achieved using Deep Lake's built-in PyTorch dataloaders `ds.pytorch()` (OSS Dataloader written in Python) or `ds.dataloader().pytorch()` (C++ and accessible to registered users). If your model training is highly sensitive to the randomization of the input data, please pre-shuffle the data, or explore our writeup on [shuffling in ds.pytorch()](https://docs.activeloop.ai/how-it-works/shuffling-in-ds.pytorch)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HP3C2uoAGnNK"
      },
      "outputs": [],
      "source": [
        "import deeplake\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "ds = deeplake.dataset('hub://activeloop/cifar100-train') # Deep Lake Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3J24ptPAyTw"
      },
      "source": [
        "The transform parameter in `ds.pytorch()` is a dictionary where the `key` is the tensor name and the `value` is the transformation function that should be applied to that tensor. If a specific tensor's data does not need to be returned, it should be omitted from the keys. If a tensor's data does not need to be modified during preprocessing, the transformation function is set as `None`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p63ojuuE6jK_"
      },
      "source": [
        "#### Transform syntax #1 - For independent transforms per tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TITOfIgV6xTE"
      },
      "source": [
        "The transform parameter in `ds.pytorch()` is a dictionary where the key is the tensor name and the value is the transformation function for that tensor. If a tensor's data does not need to be returned, the tensor should be omitted from the keys. If a tensor's data does not need to be modified during preprocessing, the transformation function for the tensor is set as `None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvqbqsCnA4P3"
      },
      "outputs": [],
      "source": [
        "tform = transforms.Compose([\n",
        "    transforms.ToPILImage(), # Must convert to PIL image for subsequent operations to run\n",
        "    transforms.RandomRotation(20), # Image augmentation\n",
        "    transforms.ToTensor(), # Must convert to pytorch tensor for subsequent operations to run\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
        "])\n",
        "\n",
        "#PyTorch Dataloader\n",
        "dataloader= ds.pytorch(batch_size = 16, num_workers = 2,\n",
        "    transform = {'images': tform, 'labels': None}, shuffle = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29RXFgme6-Gz"
      },
      "source": [
        "#### Transform syntax #2 - For complex or dependent transforms per tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXrPn-cH7Bhz"
      },
      "source": [
        "Transform are sometimes more complex where the same transform might need to be applied to all tensors, or tensors need to be combined in a transform. In this case, you can use the syntax below to perform the exact same transform as above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9gw49wS69yo"
      },
      "outputs": [],
      "source": [
        "def transform(sample_in):\n",
        "    return {'images': tform(sample_in['images']), 'labels': sample_in['labels']}\n",
        "\n",
        "#OSS PyTorch Dataloader\n",
        "dataloader_oss = ds.pytorch(batch_size = 16, num_workers = 1,\n",
        "    transform = transform, shuffle = True)\n",
        "\n",
        "\n",
        "#C++ PyTorch Dataloader\n",
        "dataloader_cpp= ds.dataloader().pytorch(num_workers = 1).transform(transform = transform).batch(batch_size = 16).shuffle(shuffle = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EQ2LUPydfPo"
      },
      "source": [
        "**Note:** Some datasets such as imagenet contain both grayscale and color images, which can cause errors when the transformed images are passed to the model. To convert only the grayscale images to color format, you can add this Torchvision transform to your pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Wi0K1aVdjMr"
      },
      "outputs": [],
      "source": [
        "# transforms.Lambda(lambda x: x.repeat(int(3/x.shape[0]), 1, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTsEinAP7HVg"
      },
      "source": [
        "### **Using PyTorch Datasets + Data Loaders**\n",
        "\n",
        "**Best option for full customizability.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX_MIn_rA70v"
      },
      "source": [
        "Deep Lake datasets can be integrated in the PyTorch Dataset class by passing the `ds` object to the PyTorch Dataset's constructor and pulling data in the `__getitem__` method using `self.ds.image[ids].numpy()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJJ5nXL98XoA"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class ClassificationDataset(Dataset):\n",
        "    def __init__(self, ds, transform = None):\n",
        "        self.ds = ds\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.ds.images[idx].numpy()\n",
        "        label = self.ds.labels[idx].numpy(fetch_chunks = True).astype(np.int32)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        sample = {\"images\": image, \"labels\": label}\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywh5-pRrHcQj"
      },
      "source": [
        "**Note:** When loading data sequentially, or when randomly loading samples from a tensor that fits into the cache (such as `class_labels`) it is recommended to set `fetch_chunks = True`. This increases the data loading speed by avoiding separate requests for each individual sample. This is not recommended when randomly loading large tensors, because the data is deleted from the cache before adjacent samples from a chunk are used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6o8zPw68iEd"
      },
      "source": [
        "The PyTorch dataset + data loader is instantiated using the built-in PyTorch functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nu39oRbt8q5r"
      },
      "outputs": [],
      "source": [
        "dataset_pytorch = ClassificationDataset(ds, transform = tform)\n",
        "\n",
        "dataloader_pytorch = DataLoader(dataset_pytorch, batch_size = 16, num_workers = 1, shuffle = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUQR2ToB8yWF"
      },
      "source": [
        "### **Iteration and Training**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EV1e8vrq88UP"
      },
      "source": [
        "You can iterate through both data loaders above using the exact same syntax. Loading the first batch of data using the Deep Lake data loader may take up to 30 seconds because the [shuffle buffer](https://docs.activeloop.ai/how-it-works/shuffling-in-ds.pytorch) is filled before any data is returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuowobdbA96I"
      },
      "outputs": [],
      "source": [
        "for data in dataloader_oss:\n",
        "    print(data)\n",
        "    break\n",
        "\n",
        "    # Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CukSSA77ulUO"
      },
      "outputs": [],
      "source": [
        "for data in dataloader_cpp:\n",
        "    print(data)\n",
        "    break\n",
        "\n",
        "    # Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xI5FWDdZ9HCT"
      },
      "outputs": [],
      "source": [
        "for data in dataloader_pytorch:\n",
        "    print(data)\n",
        "    break\n",
        "\n",
        "    # Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6cutnUI9Y-X"
      },
      "source": [
        "For more information on training, check out the tutorial on [Training and Image Classification Model in PyTorch](https://docs.activeloop.ai/tutorials/training-models/training-an-image-classification-model-in-pytorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5bX92ZUG_2F"
      },
      "source": [
        "## TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeRUG-arHP1F"
      },
      "source": [
        "Deep Lake Datasets can be converted to TensorFlow Datasets using `ds.tensorflow()`. Downstream, functions from the `tf.Data` API such as map, shuffle, etc. can be applied to process the data before training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I1bma0HSHOAO"
      },
      "outputs": [],
      "source": [
        "ds # Deep Lake Dataset object, to be used for training\n",
        "ds_tf = ds.tensorflow() # A TensorFlow Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guao84xTb4Zg"
      },
      "source": [
        "# **Step 8**: _Parallel Computing_\n",
        "\n",
        "_Running computations and processing data in parallel._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVcZ28epcKRc"
      },
      "source": [
        "Deep Lake enables you to easily run computations in parallel and significantly accelerate your data processing workflows. This example primarily focuses on parallel dataset uploading, and other use cases such as dataset transformations can be found in [this tutorial](https://docs.activeloop.ai/tutorials/data-processing-using-parallel-computing).\n",
        "\n",
        "Parallel compute using Deep Lake has two core elements: #1. defining a function or pipeline that will run in parallel and #2. evaluating it using the appropriate inputs and outputs. Let's start with #1 by defining a function that processes files and appends their data to the labels and images tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWNxzF1pcWxn"
      },
      "source": [
        "**Defining the parallel computing function**\n",
        "\n",
        "The first step for running parallel computations is to define a function that will run in parallel by decorating it using `@deeplake.compute`. In the example below, `file_to_hub` converts data from files into Deep Lake format, just like in **Step 2: Creating Deep Lake Datasets Manually**. If you have not completed Step 2, please complete the section that downloads and unzips the *animals* dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMjMF_-LcHtl"
      },
      "outputs": [],
      "source": [
        "import deeplake\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "@deeplake.compute\n",
        "def file_to_hub(file_name, sample_out, class_names):\n",
        "    ## First two arguments are always default arguments containing:\n",
        "    #     1st argument is an element of the input iterable (list, dataset, array,...)\n",
        "    #     2nd argument is a dataset sample\n",
        "    # Other arguments are optional\n",
        "\n",
        "    # Find the label number corresponding to the file\n",
        "    label_text = os.path.basename(os.path.dirname(file_name))\n",
        "    label_num = class_names.index(label_text)\n",
        "\n",
        "    # Append the label and image to the output sample\n",
        "    sample_out.append({\"labels\": np.uint32(label_num),\n",
        "                       \"images\": deeplake.read(file_name)})\n",
        "\n",
        "    return sample_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-ZhXH-pcgT8"
      },
      "source": [
        "In all functions decorated using `@deeplake.compute`, the first argument must be a single element of any input iterable that is being processed in parallel. In this case, that is a filename `file_name`, becuase `file_to_hub` reads image files and populates data in the dataset's tensors.\n",
        "\n",
        "The second argument is a dataset sample `sample_out`, which can be operated on using similar syntax to dataset objects, such as `sample_out.append(...)`, `sample_out.extend(...)`, etc.\n",
        "\n",
        "The function decorated using `@deeplake.compute` must return `sample_out`, which represents the data that is added or modified by that function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIUiNuQqchnH"
      },
      "source": [
        "**Executing the transform**\n",
        "\n",
        "To execute the transform, you must define the dataset that will be modified by the parallel computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZfEn1g_cno_"
      },
      "outputs": [],
      "source": [
        "ds = deeplake.empty('./animals_dl_transform') # Creates the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7FIReeLcpka"
      },
      "source": [
        "Next, you define the input iterable that describes the information that will be operated on in parallel. In this case, that is a list of files `files_list` from the animals dataset in Step 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CwypbTxcrx0"
      },
      "outputs": [],
      "source": [
        "# Find the class_names and list of files that need to be uploaded\n",
        "dataset_folder = './animals'\n",
        "\n",
        "class_names = os.listdir(dataset_folder)\n",
        "\n",
        "files_list = []\n",
        "for dirpath, dirnames, filenames in os.walk(dataset_folder):\n",
        "    for filename in filenames:\n",
        "        files_list.append(os.path.join(dirpath, filename))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IC-VRKVcuRI"
      },
      "source": [
        "You can now create the tensors for the dataset and **run the parallel computation** using the `.eval` syntax. Pass the optional input arguments to `file_to_hub`, and we skip the first two default arguments `file_name` and `sample_out`.\n",
        "\n",
        "The input iterable `files_list` and output dataset `ds` is passed to the `.eval` method as the first and second argument respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4H4Fug0cxJG"
      },
      "outputs": [],
      "source": [
        "with ds:\n",
        "    ds.create_tensor('images', htype = 'image', sample_compression = 'jpeg')\n",
        "    ds.create_tensor('labels', htype = 'class_label', class_names = class_names)\n",
        "\n",
        "    file_to_hub(class_names=class_names).eval(files_list, ds, num_workers = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfWc3_fkhr0W"
      },
      "outputs": [],
      "source": [
        "Image.fromarray(ds.images[0].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xTj7kt0jrd3"
      },
      "source": [
        "Congrats! You just created a dataset using parallel computing! ðŸŽˆ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXRCphquSFs3"
      },
      "source": [
        "# **Step 9**: _Dataset Version Control_\n",
        "\n",
        "*Managing changes to your datasets using Version Control.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y_V53L8SCuB"
      },
      "source": [
        "Deep Lake dataset version control allows you to manage changes to datasets with commands very similar to Git. It provides critical insights into how your data is evolving, and it works with datasets of any size!\n",
        "\n",
        "Let's check out how dataset version control works in Deep Lake! If you haven't done so already, please download and unzip the *animals* dataset from **Step 2**.\n",
        "\n",
        "First let's create a Deep Lake dataset in the `./version_control_hub` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgEWowxySUDL"
      },
      "outputs": [],
      "source": [
        "import deeplake\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Set overwrite = True for re-runability\n",
        "ds = deeplake.dataset('./version_control_hub', overwrite = True)\n",
        "\n",
        "# Create a tensor and add an image\n",
        "with ds:\n",
        "    ds.create_tensor('images', htype = 'image', sample_compression = 'jpeg')\n",
        "    ds.images.append(deeplake.read('./animals/cats/image_1.jpg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNLh-JE5pkS_"
      },
      "source": [
        "The first image in this dataset is a picture of a cat:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4hVjQaVpksW"
      },
      "outputs": [],
      "source": [
        "Image.fromarray(ds.images[0].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CEF-kjySdLp"
      },
      "source": [
        "##Commit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joKq3VV0SdEW"
      },
      "source": [
        "To commit the data added above, simply run `ds.commit`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pj9uTZeSTGwT"
      },
      "outputs": [],
      "source": [
        "first_commit_id = ds.commit('Added image of a cat')\n",
        "\n",
        "print('Dataset in commit {} has {} samples'.format(first_commit_id, len(ds)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc2-MRmaSc4x"
      },
      "source": [
        "Next, let's add another image and commit the update:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zArtG0phTZRv"
      },
      "outputs": [],
      "source": [
        "with ds:\n",
        "    ds.images.append(deeplake.read('./animals/dogs/image_3.jpg'))\n",
        "\n",
        "second_commit_id = ds.commit('Added an image of a dog')\n",
        "\n",
        "print('Dataset in commit {} has {} samples'.format(second_commit_id, len(ds)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYjnY_1RTcjM"
      },
      "source": [
        "The second image in this dataset is a picture of a dog:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbPu9JoFp0ap"
      },
      "outputs": [],
      "source": [
        "Image.fromarray(ds.images[1].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWvgUH25Tj8V"
      },
      "source": [
        "##Log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiqOb8POTkb4"
      },
      "source": [
        "The commit history starting from the current commit can be show using `ds.log`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQSxvzIcTuU-"
      },
      "outputs": [],
      "source": [
        "log = ds.log()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgefyAuATwi4"
      },
      "source": [
        "This command prints the log to the console and also assigns it to the specified variable log. The author of the commit is the username of the [Activeloop account](https://docs.activeloop.ai/getting-started/using-activeloop-storage) that logged in on the machine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JRpqeYqV-oT"
      },
      "source": [
        "##Branch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TWcOT4RV-d4"
      },
      "source": [
        "Branching takes place by running the `ds.checkout` command with the parameter `create = True`. Let's create a new branch `dog_flipped`, flip the second image (dog), and create a new commit on that branch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eY-CZmzrXr0X"
      },
      "outputs": [],
      "source": [
        "ds.checkout('dog_flipped', create = True)\n",
        "\n",
        "with ds:\n",
        "    ds.images[1] = np.transpose(ds.images[1], axes=[1,0,2])\n",
        "\n",
        "flipped_commit_id = ds.commit('Flipped the dog image')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUUMXFKEXuIq"
      },
      "source": [
        "The dog image is now flipped and the log shows a commit on the `dog_flipped` branch as well as the previous commits on `main`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIP6V3VFqPKS"
      },
      "outputs": [],
      "source": [
        "Image.fromarray(ds.images[1].numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3-UgHZPX_0u"
      },
      "outputs": [],
      "source": [
        "ds.log()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCrKgp6FYDG9"
      },
      "source": [
        "##Checkout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07nHcIIiYFtW"
      },
      "source": [
        "A previous commit of branch can be checked out using `ds.checkout`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZe8iXjlYEdf"
      },
      "outputs": [],
      "source": [
        "ds.checkout('main')\n",
        "\n",
        "Image.fromarray(ds.images[1].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AZXuEVYYVHm"
      },
      "source": [
        "As expected, the dog image on `main` is not flipped."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmydIxas3XsV"
      },
      "source": [
        "## Diff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTEuB-4C3a-B"
      },
      "source": [
        "Understanding changes between commits is critical for managing the evolution of datasets. Deep Lake's `ds.diff` function enables users to determine the number of samples that were added, removed, or updated for each tensor. The function can be used in 3 ways:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhlPmK9E37Do"
      },
      "outputs": [],
      "source": [
        "ds.diff() # Diff between the current state and the last commit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCa8-nlJ4Dxg"
      },
      "outputs": [],
      "source": [
        "ds.diff(first_commit_id) # Diff between the current state and a specific commit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bj2Yez624Ecb"
      },
      "outputs": [],
      "source": [
        "ds.diff(second_commit_id, first_commit_id) # Diff between two specific commits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1GqH1JvYkNP"
      },
      "source": [
        "##HEAD Commit\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbiRZ0eGiBrz"
      },
      "source": [
        "Unlike Git, Deep Lake's dataset version control does not have a local staging area because all dataset updates are immediately synced with the permanent storage location (cloud or local). Therefore, any changes to a dataset are automatically stored in a HEAD commit on the current branch. This means that the uncommitted changes do not appear on other branches, and uncommitted changes are visible to all users.\n",
        "\n",
        "**Let's see how this works:**\n",
        "\n",
        "You should currently be on the `main` branch, which has 2 samples. Let's adds another image:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwuzyJUViZC6"
      },
      "outputs": [],
      "source": [
        "print('Dataset on {} branch has {} samples'.format('main', len(ds)))\n",
        "\n",
        "with ds:\n",
        "    ds.images.append(deeplake.read('./animals/dogs/image_4.jpg'))\n",
        "\n",
        "print('After updating, the HEAD commit on {} branch has {} samples'.format('main', len(ds)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3qePpVFqkG9"
      },
      "source": [
        "The 3rd sample is also an image of a dog:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDfKKuhLqlMM"
      },
      "outputs": [],
      "source": [
        "Image.fromarray(ds.images[2].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4brOnBdyiq6p"
      },
      "source": [
        "Next, if you checkout `dog_flipped` branch, the dataset contains 2 samples, which is sample count from when that branch was created. Therefore, the additional uncommitted third sample that was added to the `main` branch above is not reflected when other branches or commits are checked out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvG-X9VqipM3"
      },
      "outputs": [],
      "source": [
        "ds.checkout('dog_flipped')\n",
        "\n",
        "print('Dataset in {} branch has {} samples'.format('dog_flipped', len(ds)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aoAeA7vixsC"
      },
      "source": [
        "Finally, when checking our the `main` branch again, the prior uncommitted changes and visible and they are stored in the `HEAD` commit on `main`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DnXiwTmi6G9"
      },
      "outputs": [],
      "source": [
        "ds.checkout('main')\n",
        "\n",
        "print('Dataset in {} branch has {} samples'.format('main', len(ds)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztVUV_BDqyHR"
      },
      "source": [
        "The dataset now contains 3 samples and the uncommitted dog image is visible:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ci0IHCP9q0In"
      },
      "outputs": [],
      "source": [
        "Image.fromarray(ds.images[2].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyOC3c9ZjQN3"
      },
      "source": [
        "You can delete any uncommitted changes using the `reset` command below, which will bring the `main` branch back to the state with 2 samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJqOBHk_jTGD"
      },
      "outputs": [],
      "source": [
        "ds.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCyDyEzPjSzV"
      },
      "outputs": [],
      "source": [
        "print('Dataset in {} branch has {} samples'.format('main', len(ds)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uinXs4r1i7Zz"
      },
      "source": [
        "##Merge\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQOGilvkjG2c"
      },
      "source": [
        "Merging is a critical feature for collaborating on datasets. It enables you to modify data on separate branches before making those changes available on the `main` branch, thus enabling you to experiment on your data without affecting workflows by other collaborators.\n",
        "\n",
        "We are currently on the main branch where the picture of the dog is right-side-up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bihkbZ5jccC"
      },
      "outputs": [],
      "source": [
        "ds.log()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUN7S7tAje3D"
      },
      "outputs": [],
      "source": [
        "Image.fromarray(ds.images[1].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNL3aibVjtIV"
      },
      "source": [
        "We can merge the `dog_flipped` branch into `main` using the command below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLhan9aajv6q"
      },
      "outputs": [],
      "source": [
        "ds.merge('dog_flipped')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzcJvf7zj1Yr"
      },
      "source": [
        "After merging the `dog_flipped` branch, we observe that the image of the dog is flipped. The dataset log now has a commit indicating that a commit from another branch was merged to `main`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sm9WZ2p4j4eP"
      },
      "outputs": [],
      "source": [
        "Image.fromarray(ds.images[1].numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9tUVOGoj60F"
      },
      "outputs": [],
      "source": [
        "ds.log()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz15ukH5jiIm"
      },
      "source": [
        "Congrats! You just are now an expert in dataset version control!ðŸŽ“"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBM1DKntaXwS"
      },
      "source": [
        "# **Step 10:** *Dataset Filtering*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8K5WREdaf--"
      },
      "source": [
        "Filtering and querying is an important aspect of data engineering because analyzing and utilizing data in smaller units is much more productive than executing workflows on all data all the time.\n",
        "\n",
        "Queries can be performed in Deep Lake enables with user-defined functions, or they can be executed in the [Deep Lake App](https://app.activeloop.ai/) using our highly-performance SQL-style query language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv8lkeCNl_Dt"
      },
      "source": [
        "## Filtering using our SQL query language\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk4FHtc2mBlW"
      },
      "source": [
        "[Deep Lake](https://app.activeloop.ai/) offers a highly-performant SQL-style query language that is built in C++ and is optimized for Deep Lake datasets. Queries and their results are executed and saved in the UI, and they can be accessed in Deep Lake using using the the Dataset Views API described below.\n",
        "Full details about the query language are described in a [standalone tutorial](https://docs.activeloop.ai/enterprise-features/querying-datasets)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp9i4Flsai0p"
      },
      "source": [
        "## Filtering with user-defined-functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtwDPVwmapjL"
      },
      "source": [
        "The first step for querying using UDFs is to define a function that returns a boolean depending on whether an input sample in a dataset meets the user-defined condition. In this example, we define a function that returns `True` if the labels for a tensor are in the desired `labels_list`. If there are inputs to the filtering function other than `sample_in`, it must be decorated with `@deeplake.compute`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlIFstX5mrPr"
      },
      "outputs": [],
      "source": [
        "import deeplake\n",
        "from PIL import Image\n",
        "\n",
        "# Let's create a local copy of the dataset (Explanation is in the next section)\n",
        "ds = deeplake.deepcopy('hub://activeloop/mnist-train', './mnist-train-local')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lWTjNbUamyp"
      },
      "outputs": [],
      "source": [
        "labels_list = ['0', '8'] # Desired labels for filtering\n",
        "\n",
        "@deeplake.compute\n",
        "def filter_labels(sample_in, labels_list):\n",
        "\n",
        "    return sample_in.labels.data()['text'][0] in labels_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccAa1QuZazC-"
      },
      "source": [
        "The filtering function is executed using the `ds.filter()` command below, and it returns a `Dataset View` that only contains the indices that met the filtering condition (more details below). Just like in the Parallel Computing API, the sample_in parameter does not need to be passed into the filter function when evaluating it, and multi-processing can be specified using the `scheduler` and `num_workers` parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mq4e5gRIbZ74"
      },
      "outputs": [],
      "source": [
        "ds_view = ds.filter(filter_labels(labels_list), scheduler = 'threaded', num_workers = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyJn8DZ3Z0wy"
      },
      "outputs": [],
      "source": [
        "print(len(ds_view))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ibjQKAEbs9F"
      },
      "source": [
        "**Note:** in most cases, multi-processing is not necessary for queries that involve simple data such as labels or bounding boxes. However, multi-processing significantly accelerates queries that must load rich data types such as images and videos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4_yT5ZunQcZ"
      },
      "source": [
        "## Dataset Views"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqArYgDZnWcf"
      },
      "source": [
        "A Dataset View is any subset of a Deep Lake dataset that does not contains all of the samples. It can be an output of a query, filtering function, or regular indexing like `ds[0:2:100]`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc7Y_28lnWWX"
      },
      "source": [
        "**Note:** In the filtering example above, we copied mnist-train locally in order to gain write access to the dataset, which affects the saving of `Dataset Views`. With write access, the views are saved as part of the dataset. Without write access, views are stored elsewhere or in custom paths, and full details are available here. Users have write access to their own datasets, regardless of whether the datasets are local or in the cloud."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2DowEHfbb2m"
      },
      "source": [
        "The data in the returned `ds_view` can be accessed just like a regular dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FCxLH0Nbec8"
      },
      "outputs": [],
      "source": [
        "Image.fromarray(ds_view.images[0].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8NxyIw2nnWY"
      },
      "source": [
        "A `Dataset View` can be saved permanently using the method below, which stores its indices without copying the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzmlwsJLnmfQ"
      },
      "outputs": [],
      "source": [
        "ds_view.save_view(message = 'Samples with 0 and 8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyaFzRE6n50w"
      },
      "source": [
        "**Note:** In order to maintain data lineage, Dataset Views are immutable and are connected to specific commits. Therefore, views can only be saved if the dataset has a commit and there are no uncommitted changes in the HEAD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLB1sM50Y4ek"
      },
      "source": [
        "Each `Dataset View` has a unique `id`, and views can be examined or loaded using:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzQ1foyWY3yE"
      },
      "outputs": [],
      "source": [
        "views = ds.get_views()\n",
        "\n",
        "print(views)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PKOwe5qY8zt"
      },
      "outputs": [],
      "source": [
        "ds_view = views[-1].load()\n",
        "\n",
        "# OR\n",
        "\n",
        "# ds_view = ds.load_view(id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVuY_x35anoW"
      },
      "outputs": [],
      "source": [
        "print(len(ds_view))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEi-DxIOcpHp"
      },
      "source": [
        "Congrats! You just learned to filter adn query data with Deep Lake! ðŸŽˆ"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "lKU8kmSs65xv",
        "G-DM6PKq_di2",
        "46H4nEnZDv5m",
        "JGo-E8Z8Ho6F",
        "K385Fpvmqc0l",
        "NQipSo2OF_lB",
        "LVma__gxGq97",
        "Bnr9ItdkGzDk",
        "x5bX92ZUG_2F",
        "guao84xTb4Zg",
        "iXRCphquSFs3",
        "vBM1DKntaXwS"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}